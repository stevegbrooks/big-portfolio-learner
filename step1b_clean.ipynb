{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevegbrooks/big-portfolio-learner/blob/step1b/step1b_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuVdqztTGMiw"
      },
      "source": [
        "# CIS 545 Final Project\n",
        "\n",
        "## Big Portfolio Learner: Clean Up Technical Indicators Data\n",
        "\n",
        "### Team members: Steven Brooks & Chenlia Xu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ccEPNzH-QeHL"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np \n",
        "import json\n",
        "import matplotlib\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from datetime import datetime\n",
        "import glob\n",
        "import seaborn as sns\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3zaggIhAZzeK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "## If boto3 not already installed uncomment the following:\n",
        "!pip3 install boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dkkvY7IDQ-ou"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "\n",
        "from botocore.config import Config\n",
        "\n",
        "s3 = boto3.resource('s3', config=Config(signature_version=UNSIGNED))\n",
        "s3.Bucket('cis545project').download_file('data/stock_data.zip', 'stock_data.zip')\n",
        "s3.Bucket('cis545project').download_file('data/technical_data.zip', 'technical_data.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "775OUJLwkKkn"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "stock_dir = \"stock_data\"\n",
        "if not os.path.exists(stock_dir):\n",
        "  os.makedirs(stock_dir)\n",
        "!unzip /content/stock_data.zip -d /content/$stock_dir\n",
        "!rm -f $stock_dir/.gitempty\n",
        "\n",
        "tech_dir = \"technical_data\"\n",
        "if not os.path.exists(tech_dir):\n",
        "  os.makedirs(tech_dir)\n",
        "!unzip /content/technical_data.zip -d /content/$tech_dir\n",
        "!rm -f $tech_dir/.gitempty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXFVBKfoZR5V"
      },
      "source": [
        "# Setup Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qvwYWLqRuc0Z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!apt install libkrb5-dev\n",
        "!pip install findspark\n",
        "!pip install sparkmagic\n",
        "!pip install pyspark\n",
        "!pip install pyspark --user\n",
        "!pip install seaborn --user\n",
        "!pip install imageio --user\n",
        "!pip install folium --user\n",
        "\n",
        "!apt update\n",
        "!apt install gcc python-dev libkrb5-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ygqKoO0Lusl3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "s4cFr7oquvNb",
        "outputId": "b47095ae-1ccf-43f9-b44f-8953f57ff637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ],
      "source": [
        "%load_ext sparkmagic.magics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-hpTht6CZRKw"
      },
      "outputs": [],
      "source": [
        "#graph section\n",
        "import networkx as nx\n",
        "# SQLite RDBMS\n",
        "import sqlite3\n",
        "# Parallel processing\n",
        "# import swifter\n",
        "import pandas as pd\n",
        "# NoSQL DB\n",
        "from pymongo import MongoClient\n",
        "from pymongo.errors import DuplicateKeyError, OperationFailure\n",
        "\n",
        "import os\n",
        "os.environ['SPARK_HOME'] = '/content/spark-2.4.5-bin-hadoop2.7'\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "import pyspark\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "try:\n",
        "    if(spark == None):\n",
        "        spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "        sqlContext=SQLContext(spark)\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "    sqlContext=SQLContext(spark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSmPBwAUnHlY"
      },
      "source": [
        "We will load the data into the Spark context here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "C5iYQ3VZnGxp"
      },
      "outputs": [],
      "source": [
        "stock_data_sdf = spark.read.load(\n",
        "    'stock_data/*.csv', \n",
        "    format = 'csv', \n",
        "    header = 'true', \n",
        "    inferSchema = 'true', \n",
        "    sep = ','\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pERX_hPE0634"
      },
      "source": [
        "First we'll set up the spark dataframe for stock prices using the work in `step1a`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_AfnFZqh06EY"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import log, to_timestamp\n",
        "from pyspark.sql.functions import year, month, date_format\n",
        "\n",
        "\n",
        "stock_data_sdf = stock_data_sdf.withColumn(\"timestamp_as_dt\", to_timestamp(stock_data_sdf.timestamp, 'yyyy-MM-dd'))\n",
        "stock_data_sdf = stock_data_sdf.withColumn(\"year\", year('timestamp_as_dt'))\n",
        "stock_data_sdf = stock_data_sdf.withColumn(\"adjusted_close_log\", log(\"adjusted_close\"))\n",
        "stock_data_sdf = stock_data_sdf.filter(\"year >= 2002 AND year <= 2019\")\n",
        "\n",
        "count_by_symbol_year_sdf = stock_data_sdf.groupBy([\"symbol\", \"year\"]).count()\n",
        "count_years_by_symbol_sdf = count_by_symbol_year_sdf.groupBy([\"symbol\"]).count()\n",
        "\n",
        "### Just grab stocks that have data in each of the 18 years from 2002 to 2019\n",
        "### AND remove the three outliers\n",
        "stocks_with_all_analysis_yrs_sdf = count_years_by_symbol_sdf.filter(\"count == 18\") #18 years of data from 2002 and 2019\n",
        "stocks_to_remove = ['DCTH', 'BRK-A', 'AIKI']\n",
        "stocks_with_all_analysis_yrs_sdf = stocks_with_all_analysis_yrs_sdf.filter(stocks_with_all_analysis_yrs_sdf.symbol.isin(stocks_to_remove) == False)\n",
        "\n",
        "stock_data_sdf.createOrReplaceTempView(\"stock_data\")\n",
        "stocks_with_all_analysis_yrs_sdf.createOrReplaceTempView(\"stocks_with_all_analysis_yrs\")\n",
        "\n",
        "stock_data_2002_2019_sdf = spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT *\n",
        "    FROM stock_data\n",
        "    WHERE symbol IN (SELECT symbol FROM stocks_with_all_analysis_yrs)\n",
        "    \"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "68-WmMFdvKJ5"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructField, DoubleType, StructType, StringType\n",
        "\n",
        "structs=[\n",
        "  StructField('symbol', StringType(), True),\n",
        "  StructField('timestamp', StringType(), True),\n",
        "  StructField('EMA', DoubleType(), True),\n",
        "  StructField('MACD', DoubleType(), True),\n",
        "  StructField('SlowD', DoubleType(), True),\n",
        "  StructField('RSI', DoubleType(), True),\n",
        "  StructField('Real_Middle_Band', DoubleType(), True)\n",
        "]\n",
        "\n",
        "schema = StructType(fields = structs)\n",
        "\n",
        "technical_data_sdf = spark.read.load(\n",
        "    'technical_data/*.csv', \n",
        "    format = 'csv', \n",
        "    header = 'true', \n",
        "    schema = schema, \n",
        "    sep = ','\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7cU8zpR6JdR"
      },
      "source": [
        "Then we will reduce the technical data set to just those stocks that match up with the first dataset above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GHI-HU7l6KjK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c42b0c6-53ae-43f7-aa98-b67efcde1ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- symbol: string (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- EMA: double (nullable = true)\n",
            " |-- MACD: double (nullable = true)\n",
            " |-- SlowD: double (nullable = true)\n",
            " |-- RSI: double (nullable = true)\n",
            " |-- Real_Middle_Band: double (nullable = true)\n",
            " |-- timestamp_as_dt: timestamp (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "technical_data_sdf = technical_data_sdf.withColumn(\"timestamp_as_dt\", to_timestamp(technical_data_sdf.timestamp, 'yyyy-MM-dd'))\n",
        "technical_data_sdf = technical_data_sdf.withColumn(\"year\", year('timestamp_as_dt'))\n",
        "technical_data_sdf = technical_data_sdf.filter(\"year >= 2002 AND year <= 2019\")\n",
        "\n",
        "technical_data_sdf.createOrReplaceTempView(\"technical_data\")\n",
        "\n",
        "technical_data_2002_2019_sdf = spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT *\n",
        "    FROM technical_data\n",
        "    WHERE symbol IN (SELECT symbol FROM stocks_with_all_analysis_yrs)\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "technical_data_2002_2019_sdf.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join the two datasets and split for train, validation, and test"
      ],
      "metadata": {
        "id": "Im_2Tbttv_XX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "technical_data_2002_2019_sdf.createOrReplaceTempView(\"cleaned_technical_data\")\n",
        "stock_data_2002_2019_sdf.createOrReplaceTempView(\"cleaned_stock_data\")"
      ],
      "metadata": {
        "id": "5LcbZc9yv5cJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT T.symbol, T.timestamp_as_dt, T.EMA, T.MACD, T.SlowD, T.RSI, T.Real_Middle_Band, S.volume\n",
        "    FROM cleaned_technical_data T JOIN cleaned_stock_data S\n",
        "    ON T.symbol = S.symbol AND T.timestamp_as_dt = S.timestamp_as_dt\n",
        "    \"\"\"\n",
        ").createOrReplaceTempView(\"cleaned_technical_data\")"
      ],
      "metadata": {
        "id": "mfuvFneewX-6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT symbol, timestamp_as_dt, adjusted_close_log\n",
        "    FROM cleaned_stock_data\n",
        "    \"\"\"\n",
        ").createOrReplaceTempView(\"cleaned_stock_data\")"
      ],
      "metadata": {
        "id": "3O2JmEH4w3xi"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6gRnnA4ZxKsu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "step1b_clean.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}