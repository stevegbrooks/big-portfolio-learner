{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevegbrooks/big-portfolio-learner/blob/main/step1b_clean_and_combine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuVdqztTGMiw"
      },
      "source": [
        "# CIS 545 Final Project\n",
        "\n",
        "## Big Portfolio Learner: Clean Up Technical Indicators Data\n",
        "\n",
        "### Team members: Steven Brooks & Chenlia Xu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccEPNzH-QeHL"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np \n",
        "import json\n",
        "import matplotlib\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from datetime import datetime\n",
        "import glob\n",
        "import seaborn as sns\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zaggIhAZzeK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "## If boto3 not already installed uncomment the following:\n",
        "!pip3 install boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkkvY7IDQ-ou"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "\n",
        "from botocore.config import Config\n",
        "\n",
        "s3 = boto3.resource('s3', config=Config(signature_version=UNSIGNED))\n",
        "s3.Bucket('cis545project').download_file('data/stock_data.zip', 'stock_data.zip')\n",
        "s3.Bucket('cis545project').download_file('data/technical_data.zip', 'technical_data.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "775OUJLwkKkn"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "stock_dir = \"stock_data\"\n",
        "if not os.path.exists(stock_dir):\n",
        "  os.makedirs(stock_dir)\n",
        "!unzip /content/stock_data.zip -d /content/$stock_dir\n",
        "!rm -f $stock_dir/.gitempty\n",
        "\n",
        "tech_dir = \"technical_data\"\n",
        "if not os.path.exists(tech_dir):\n",
        "  os.makedirs(tech_dir)\n",
        "!unzip /content/technical_data.zip -d /content/$tech_dir\n",
        "!rm -f $tech_dir/.gitempty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXFVBKfoZR5V"
      },
      "source": [
        "# Setup Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvwYWLqRuc0Z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!apt install libkrb5-dev\n",
        "!pip install findspark\n",
        "!pip install sparkmagic\n",
        "!pip install pyspark\n",
        "!pip install pyspark --user\n",
        "!pip install seaborn --user\n",
        "!pip install imageio --user\n",
        "!pip install folium --user\n",
        "\n",
        "!apt update\n",
        "!apt install gcc python-dev libkrb5-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygqKoO0Lusl3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4cFr7oquvNb",
        "outputId": "220b8a80-2956-444a-e893-8e3b7cc1eee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sparkmagic.magics extension is already loaded. To reload it, use:\n",
            "  %reload_ext sparkmagic.magics\n"
          ]
        }
      ],
      "source": [
        "%load_ext sparkmagic.magics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hpTht6CZRKw"
      },
      "outputs": [],
      "source": [
        "#graph section\n",
        "import networkx as nx\n",
        "# SQLite RDBMS\n",
        "import sqlite3\n",
        "# Parallel processing\n",
        "# import swifter\n",
        "import pandas as pd\n",
        "# NoSQL DB\n",
        "from pymongo import MongoClient\n",
        "from pymongo.errors import DuplicateKeyError, OperationFailure\n",
        "\n",
        "import os\n",
        "os.environ['SPARK_HOME'] = '/content/spark-2.4.5-bin-hadoop2.7'\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "import pyspark\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "try:\n",
        "    if(spark == None):\n",
        "        spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "        sqlContext=SQLContext(spark)\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "    sqlContext=SQLContext(spark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSmPBwAUnHlY"
      },
      "source": [
        "We will load the data into the Spark context here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5iYQ3VZnGxp",
        "outputId": "fc6c1eb0-4ac8-4b17-84d0-f03a45b03673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- symbol: string (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- adjusted_close: double (nullable = true)\n",
            " |-- volume: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stock_data_sdf = spark.read.load(\n",
        "    'stock_data/*.csv', \n",
        "    format = 'csv', \n",
        "    header = 'true', \n",
        "    inferSchema = 'true', \n",
        "    sep = ','\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we'll set up the spark dataframe for stock prices using the work in `step1a`."
      ],
      "metadata": {
        "id": "pERX_hPE0634"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_timestamp\n",
        "from pyspark.sql.functions import year, month, date_format\n",
        "\n",
        "\n",
        "stock_data_sdf = stock_data_sdf.withColumn(\"timestamp_as_dt\", to_timestamp(stock_data_sdf.timestamp, 'yyyy-MM-dd'))\n",
        "stock_data_sdf = stock_data_sdf.withColumn(\"year\", year('timestamp_as_dt'))\n",
        "stock_data_sdf = stock_data_sdf.filter(\"year >= 2002 AND year <= 2019\")\n",
        "\n",
        "count_by_symbol_year_sdf = stock_data_sdf.groupBy([\"symbol\", \"year\"]).count()\n",
        "count_years_by_symbol_sdf = count_by_symbol_year_sdf.groupBy([\"symbol\"]).count()\n",
        "\n",
        "### Just grab stocks that have data in each of the 18 years from 2002 to 2019\n",
        "### AND remove the three outliers\n",
        "stocks_with_all_analysis_yrs_sdf = count_years_by_symbol_sdf.filter(\"count == 18\") #18 years of data from 2002 and 2019\n",
        "stocks_to_remove = ['DCTH', 'BRK-A', 'AIKI']\n",
        "stocks_with_all_analysis_yrs_sdf = stocks_with_all_analysis_yrs_sdf.filter(stocks_with_all_analysis_yrs_sdf.symbol.isin(stocks_to_remove) == False)\n",
        "\n",
        "stock_data_sdf.createOrReplaceTempView(\"stock_data\")\n",
        "stocks_with_all_analysis_yrs_sdf.createOrReplaceTempView(\"stocks_with_all_analysis_yrs\")\n",
        "\n",
        "stock_data_2002_2019_sdf = spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT *\n",
        "    FROM stock_data\n",
        "    WHERE symbol IN (SELECT symbol FROM stocks_with_all_analysis_yrs)\n",
        "    \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "_AfnFZqh06EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "technical_data_sdf = spark.read.load(\n",
        "    'technical_data/*.csv', \n",
        "    format = 'csv', \n",
        "    header = 'true', \n",
        "    inferSchema = 'true', \n",
        "    sep = ','\n",
        ")"
      ],
      "metadata": {
        "id": "68-WmMFdvKJ5",
        "outputId": "e9165e4e-d413-4632-8729-ca6cb1f88529",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- symbol: string (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- EMA: string (nullable = true)\n",
            " |-- MACD: string (nullable = true)\n",
            " |-- MACD_Hist: string (nullable = true)\n",
            " |-- MACD_Signal: string (nullable = true)\n",
            " |-- SlowD: string (nullable = true)\n",
            " |-- SlowK: string (nullable = true)\n",
            " |-- RSI: string (nullable = true)\n",
            " |-- Real_Lower_Band: string (nullable = true)\n",
            " |-- Real_Middle_Band: string (nullable = true)\n",
            " |-- Real_Upper_Band: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will reduce the technical data set to just those stocks that match up with the first dataset above."
      ],
      "metadata": {
        "id": "s7cU8zpR6JdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "technical_data_sdf = technical_data_sdf.withColumn(\"timestamp_as_dt\", to_timestamp(technical_data_sdf.timestamp, 'yyyy-MM-dd'))\n",
        "technical_data_sdf = technical_data_sdf.withColumn(\"year\", year('timestamp_as_dt'))\n",
        "technical_data_sdf = technical_data_sdf.filter(\"year >= 2002 AND year <= 2019\")\n",
        "\n",
        "technical_data_sdf.createOrReplaceTempView(\"technical_data\")\n",
        "\n",
        "technical_data_2002_2019_sdf = spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT *\n",
        "    FROM technical_data\n",
        "    WHERE symbol IN (SELECT symbol FROM stocks_with_all_analysis_yrs)\n",
        "    \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "GHI-HU7l6KjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step is to clean up technical data (especially fixing the schema so its numeric instead of string) and then setting it up for modeling."
      ],
      "metadata": {
        "id": "06Q9uomP8824"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LXt9KRZl6tCa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "step1b_clean_and_combine.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}