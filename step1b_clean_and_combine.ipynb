{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevegbrooks/big-portfolio-learner/blob/big-data/step1b_clean_and_combine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuVdqztTGMiw"
      },
      "source": [
        "# CIS 545 Final Project\n",
        "\n",
        "## Big Portfolio Learner: Clean Up Technical Indicators Data and Combine with Price Data\n",
        "\n",
        "### Team members: Steven Brooks & Chenlia Xu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccEPNzH-QeHL"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np \n",
        "import json\n",
        "import matplotlib\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from datetime import datetime\n",
        "import glob\n",
        "import seaborn as sns\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zaggIhAZzeK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "## If boto3 not already installed uncomment the following:\n",
        "!pip3 install boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkkvY7IDQ-ou"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "\n",
        "from botocore.config import Config\n",
        "\n",
        "s3 = boto3.resource('s3', config=Config(signature_version=UNSIGNED))\n",
        "s3.Bucket('cis545project').download_file('data/stock_data_2002_2019.csv', 'stock_data_2002_2019.csv')\n",
        "s3.Bucket('cis545project').download_file('data/technical_data.zip', 'technical_data.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "775OUJLwkKkn"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "tech_dir = \"technical_data\"\n",
        "if not os.path.exists(tech_dir):\n",
        "  os.makedirs(tech_dir)\n",
        "!unzip /content/technical_data.zip -d /content/$tech_dir\n",
        "!rm -f $tech_dir/.gitempty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXFVBKfoZR5V"
      },
      "source": [
        "# Setup Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvwYWLqRuc0Z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!apt install libkrb5-dev\n",
        "!pip install findspark\n",
        "!pip install sparkmagic\n",
        "!pip install pyspark\n",
        "!pip install pyspark --user\n",
        "!pip install seaborn --user\n",
        "!pip install imageio --user\n",
        "!pip install folium --user\n",
        "\n",
        "!apt update\n",
        "!apt install gcc python-dev libkrb5-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygqKoO0Lusl3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4cFr7oquvNb"
      },
      "outputs": [],
      "source": [
        "%load_ext sparkmagic.magics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hpTht6CZRKw"
      },
      "outputs": [],
      "source": [
        "#graph section\n",
        "import networkx as nx\n",
        "# SQLite RDBMS\n",
        "import sqlite3\n",
        "# Parallel processing\n",
        "# import swifter\n",
        "import pandas as pd\n",
        "# NoSQL DB\n",
        "from pymongo import MongoClient\n",
        "from pymongo.errors import DuplicateKeyError, OperationFailure\n",
        "\n",
        "import os\n",
        "os.environ['SPARK_HOME'] = '/content/spark-2.4.5-bin-hadoop2.7'\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "import pyspark\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "try:\n",
        "    if(spark == None):\n",
        "        spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "        sqlContext=SQLContext(spark)\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "    sqlContext=SQLContext(spark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSmPBwAUnHlY"
      },
      "source": [
        "We will load the data into the Spark context here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5iYQ3VZnGxp",
        "outputId": "1eba3ad5-1986-4db8-b671-eef04e5192b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- symbol: string (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- adjusted_close: double (nullable = true)\n",
            " |-- volume: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stock_data_sdf = spark.read.load(\n",
        "    'stock_data/*.csv', \n",
        "    format = 'csv', \n",
        "    header = 'true', \n",
        "    inferSchema = 'true', \n",
        "    sep = ','\n",
        ")\n",
        "\n",
        "stock_data_sdf.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcgsCH66oEfZ"
      },
      "source": [
        "# Section 1: Train Test Split\n",
        "\n",
        "We will train the data using the years 2002 to 2017. Our validation set will be the year 2018. Our test set will be the year 2019."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "step1b_clean_and_combine.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
